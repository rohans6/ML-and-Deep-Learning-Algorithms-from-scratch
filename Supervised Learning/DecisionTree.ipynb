{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris,load_boston\n",
    "from sklearn.metrics import accuracy_score,r2_score\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decision_node():\n",
    "    def __init__(self,feature_i=None,threshold=None,value=None,true_branch=None,false_branch=None):\n",
    "        self.feature_i=feature_i\n",
    "        self.threshold=threshold\n",
    "        self.value=value\n",
    "        self.true_branch=true_branch\n",
    "        self.false_branch=false_branch\n",
    "class Decision_tree(object):\n",
    "    def __init__(self,min_samples_split=2,max_depth=float(\"inf\"),min_impurity=1e-7,loss=None):\n",
    "        self.min_samples_split=min_samples_split\n",
    "        self.max_depth=max_depth\n",
    "        self.min_impurity=min_impurity\n",
    "        self.loss=loss\n",
    "        self.impurity_calculation=None\n",
    "        self.leaf_calculation=None\n",
    "        self.one_dim=None\n",
    "    def fit(self,X,y,loss=None):\n",
    "        self.root=self.create_tree(X,y)\n",
    "        self.loss=None\n",
    "    def create_tree(self,X,y,current_depth=0):\n",
    "        max_impurity=0\n",
    "        best_criteria=None\n",
    "        best_sets=None\n",
    "        if len(np.shape(y)) == 1:\n",
    "            y = np.expand_dims(y, axis=1)\n",
    "\n",
    "        # Add y as last column of X\n",
    "        Xy = np.concatenate((X, y), axis=1)\n",
    "        samples,features=np.shape(X)\n",
    "        if(samples>self.min_samples_split and current_depth<=self.max_depth):\n",
    "            for feature_i in range(features):\n",
    "                feature_values=np.expand_dims(X[:,feature_i],axis=1)\n",
    "                unique_values=np.unique(feature_values)\n",
    "                for threshold in unique_values:\n",
    "                    x1,x2=divide_on_feature(Xy,feature_i,threshold)\n",
    "                    if(len(x1)>0 and len(x2)>0):\n",
    "                        y1=x1[:,features:]\n",
    "                        y2=x2[:,features:]\n",
    "                        impurity=self.impurity_calculation(y,y1,y2)\n",
    "                        if(impurity>max_impurity):\n",
    "                            max_impurity=impurity\n",
    "                            best_criteria={\"feature_i\":feature_i,\"threshold\":threshold}\n",
    "                            best_sets={\"left_X\":x1[:,:features],\n",
    "                                      \"left_y\":x1[:,features:],\n",
    "                                      \"rigth_X\":x2[:,:features],\n",
    "                                      \"right_y\":x2[:,features:]}\n",
    "            \n",
    "        if(max_impurity>self.min_impurity):\n",
    "            true_branch=self.create_tree(best_sets[\"left_X\"],best_sets[\"left_y\"],current_depth+1)\n",
    "            false_branch=self.create_tree(best_sets[\"rigth_X\"],best_sets[\"right_y\"],current_depth+1)\n",
    "            return Decision_node(feature_i=best_criteria[\"feature_i\"],threshold=best_criteria[\"threshold\"],true_branch=true_branch,false_branch=false_branch)\n",
    "        leaf_value=self.leaf_calculation(y)\n",
    "        return Decision_node(value=leaf_value)\n",
    "    \n",
    "\n",
    "    def predict_value(self, x, tree=None):\n",
    "        \"\"\" Do a recursive search down the tree and make a prediction of the data sample by the\n",
    "            value of the leaf that we end up at \"\"\"\n",
    "\n",
    "        if tree is None:\n",
    "            tree = self.root\n",
    "\n",
    "        # If we have a value (i.e we're at a leaf) => return value as the prediction\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "\n",
    "        # Choose the feature that we will test\n",
    "        feature_value = x[tree.feature_i]\n",
    "\n",
    "        # Determine if we will follow left or right branch\n",
    "        branch = tree.false_branch\n",
    "        if isinstance(feature_value, int) or isinstance(feature_value, float):\n",
    "            if feature_value >= tree.threshold:\n",
    "                branch = tree.true_branch\n",
    "        elif feature_value == tree.threshold:\n",
    "            branch = tree.true_branch\n",
    "\n",
    "        # Test subtree\n",
    "        return self.predict_value(x, branch)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Classify samples one by one and return the set of labels \"\"\"\n",
    "        y_pred = [self.predict_value(sample) for sample in X]\n",
    "        return y_pred\n",
    "class RegressionTree(Decision_tree):\n",
    "    def _calculate_variance_reduction(self, y, y1, y2):\n",
    "        var_tot = calculate_variance(y)\n",
    "        var_1 = calculate_variance(y1)\n",
    "        var_2 = calculate_variance(y2)\n",
    "        frac_1 = len(y1) / len(y)\n",
    "        frac_2 = len(y2) / len(y)\n",
    "\n",
    "        # Calculate the variance reduction\n",
    "        variance_reduction = var_tot - (frac_1 * var_1 + frac_2 * var_2)\n",
    "\n",
    "        return sum(variance_reduction)\n",
    "\n",
    "    def _mean_of_y(self, y):\n",
    "        value = np.mean(y, axis=0)\n",
    "        return value if len(value) > 1 else value[0]\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.impurity_calculation = self._calculate_variance_reduction\n",
    "        self.leaf_calculation = self._mean_of_y\n",
    "        super(RegressionTree, self).fit(X, y)\n",
    "class DecisionClassification(Decision_tree):\n",
    "    def _calculate_information_gain(self, y, y1, y2):\n",
    "        # Calculate information gain\n",
    "        p = len(y1) / len(y)\n",
    "        entropy = calculate_entropy(y)\n",
    "        info_gain = entropy - p * \\\n",
    "            calculate_entropy(y1) - (1 - p) * \\\n",
    "            calculate_entropy(y2)\n",
    "\n",
    "        return info_gain\n",
    "    def _majority_vote(self, y):\n",
    "        most_common = None\n",
    "        max_count = 0\n",
    "        for label in np.unique(y):\n",
    "            # Count number of occurences of samples with label\n",
    "            count = len(y[y == label])\n",
    "            if count > max_count:\n",
    "                most_common = label\n",
    "                max_count = count\n",
    "        return most_common\n",
    "    def fit(self,X,y):\n",
    "        self.impurity_calculation=self._calculate_information_gain\n",
    "        self.leaf_calculation=self._majority_vote\n",
    "        super(DecisionClassification,self).fit(X,y)\n",
    "            \n",
    "        \n",
    "      \n",
    "        \n",
    "                            \n",
    "                    \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_on_feature(X,feature_i,threshold):\n",
    "    sub=None\n",
    "    if(isinstance(threshold,int) or isinstance(threshold,float)):\n",
    "        sub=lambda sample: sample[feature_i]>=threshold\n",
    "    else:\n",
    "        sub=lambda sample:sample[feature_i]==threshold\n",
    "    x1=[sample for sample in X if sub(sample)]\n",
    "    x2=[sample for sample in X if not sub(sample)]\n",
    "    return np.array(x1),np.array(x2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_variance(X):\n",
    "\n",
    "    \"\"\" Return the variance of the features in dataset X \"\"\"\n",
    "\n",
    "    mean = np.ones(np.shape(X)) * X.mean(0)\n",
    "\n",
    "    n_samples = np.shape(X)[0]\n",
    "\n",
    "    variance = (1 / n_samples) * np.diag((X - mean).T.dot(X - mean))\n",
    "\n",
    "    \n",
    "\n",
    "    return variance\n",
    "                            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def calculate_entropy(y):\n",
    "\n",
    "    \"\"\" Calculate the entropy of label array y \"\"\"\n",
    "\n",
    "    log2 = lambda x: math.log(x) / math.log(2)\n",
    "\n",
    "    unique_labels = np.unique(y)\n",
    "\n",
    "    entropy = 0\n",
    "\n",
    "    for label in unique_labels:\n",
    "\n",
    "        count = len(y[y == label])\n",
    "\n",
    "        p = count / len(y)\n",
    "\n",
    "        entropy += -p * log2(p)\n",
    "\n",
    "    return entropy    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=load_boston().data,load_boston().target\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=RegressionTree()\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8557004172198377"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9910714285714286\n",
      "0.8947368421052632\n"
     ]
    }
   ],
   "source": [
    "x,Y=load_iris().data,load_iris().target\n",
    "x_train,x_test,Y_train,Y_test=train_test_split(x,Y)\n",
    "clf=DecisionClassification()\n",
    "clf.fit(x_train,Y_train)\n",
    "pred1=clf.predict(x_train)\n",
    "pred_test1=clf.predict(x_test)\n",
    "print(accuracy_score(Y_train,pred1))\n",
    "print(accuracy_score(Y_test,pred_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
